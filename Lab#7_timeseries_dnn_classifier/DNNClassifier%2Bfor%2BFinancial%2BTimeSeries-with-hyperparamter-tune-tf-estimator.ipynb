{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Predict whether S&P 500 will close higher or lower than yesterday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this solution, you will:\n",
    "\n",
    "* Obtain data for a number of financial markets.\n",
    "* Wrangle that data into a usable format and perform exploratory data analysis in order to explore and validate a premise.\n",
    "* Use TensorFlow on Cloud ML Engine to build, train and evaluate a number of models for predicting what will happen in financial markets\n",
    "\n",
    "**Important:** This solution is intended to illustrate the capabilities of GCP and TensorFlow for fast, interactive, iterative data analysis and machine learning. It does not offer any advice on financial markets or trading strategies. The scenario presented in the tutorial is an example. Don't use this code to make investment decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Prerequisites\n",
    "**Note:** You do not need to download and run your own copy of this notebook on Cloud Datalab to read and derive value, but if you want to make changes and experiment on your own, which is strongly encouraged, you will need to complete these prerequisites.\n",
    "\n",
    "You can use your free $300 trial that Google Cloud provides for new accounts\n",
    "* [Create a project](https://cloud.google.com/docs/overview/)\n",
    "* [Enable billing for you project](https://console.developers.google.com/billing).\n",
    "* [Launch Cloud Datalab for your project](https://cloud.google.com/datalab/getting-started)\n",
    "* This notebook is included in the Cloud Datalab distribution in the ‘notebooks’ directory.  Simply open the notebook and proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## The premise\n",
    "The premise is straightforward: financial markets are increasingly global, and if you follow the sun from Asia to Europe to the US and so on, you can use information from an earlier time zone to your advantage in a later time zone.\n",
    "\n",
    "The following table shows a number of stock market indices from around the globe, their closing times in Eastern Standard Time (EST), and the delay in hours between the close that index and the close of the S&P 500 in New York. This makes EST the base time zone. For example, Australian markets close for the day 15 hours before US markets close. If the close of the All Ords in Australia is a useful predictor of the close of the S&P 500 for a given day we can use that information to guide our trading activity. Continuing our example of the Australian All Ords, if this index closes up and we think that means the S&P 500 will close up as well then we should either buy stocks that compose the S&P 500 or, more likely, an ETF that tracks the S&P 500. In reality, the situation is more complex because there are commissions and tax to account for. But as a first approximation, we'll assume an index closing up indicates a gain, and vice-versa.\n",
    "\n",
    "|Index|Country|Closing Time (EST)|Hours Before S&P Close|\n",
    "|---|---|---|---|\n",
    "|[All Ords](https://en.wikipedia.org/wiki/All_Ordinaries)|Australia|0100|15|\n",
    "|[Nikkei 225](https://en.wikipedia.org/wiki/Nikkei_225)|Japan|0200|14|\n",
    "|[Hang Seng](https://en.wikipedia.org/wiki/Hang_Seng_Index)|Hong Kong|0400|12|\n",
    "|[DAX](https://en.wikipedia.org/wiki/DAX)|Germany|1130|4.5|\n",
    "|[FTSE 100](https://en.wikipedia.org/wiki/FTSE_100_Index)|UK|1130|4.5|\n",
    "|[NYSE Composite](https://en.wikipedia.org/wiki/NYSE_Composite)|US|1600|0|\n",
    "|[Dow Jones Industrial Average](https://en.wikipedia.org/wiki/Dow_Jones_Industrial_Average)|US|1600|0|\n",
    "|[S&P 500](https://en.wikipedia.org/wiki/S%26P_500_Index)|US|1600|0|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Set Up: Imports (The packages already come installed with Datalab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.tools.plotting import autocorrelation_plot\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import google.datalab.bigquery as bq\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Verify TensorFlow Version\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Get the Data from [BigQuery](https://cloud.google.com/bigquery) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Querying BigQuery to get data from Public dataset available on a public project (bingo-ml-1) with dataset named market_data\n",
    "snp = bq.Query(\"SELECT Date, Close FROM `bingo-ml-1.market_data.snp` ORDER BY Date\").execute().result().to_dataframe().set_index('Date')\n",
    "nyse = bq.Query(\"SELECT Date, Close FROM `bingo-ml-1.market_data.nyse` ORDER BY Date\").execute().result().to_dataframe().set_index('Date')\n",
    "djia = bq.Query(\"SELECT Date, Close FROM `bingo-ml-1.market_data.djia` ORDER BY Date\").execute().result().to_dataframe().set_index('Date')\n",
    "nikkei = bq.Query(\"SELECT Date, Close FROM `bingo-ml-1.market_data.nikkei` ORDER BY Date\").execute().result().to_dataframe().set_index('Date')\n",
    "hangseng = bq.Query(\"SELECT Date, Close FROM `bingo-ml-1.market_data.hangseng` ORDER BY Date\").execute().result().to_dataframe().set_index('Date')\n",
    "ftse = bq.Query(\"SELECT Date, Close FROM `bingo-ml-1.market_data.ftse` ORDER BY Date\").execute().result().to_dataframe().set_index('Date')\n",
    "dax = bq.Query(\"SELECT Date, Close FROM `bingo-ml-1.market_data.dax` ORDER BY Date\").execute().result().to_dataframe().set_index('Date')\n",
    "aord = bq.Query(\"SELECT Date, Close FROM `bingo-ml-1.market_data.aord` ORDER BY Date\").execute().result().to_dataframe().set_index('Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Lets quickly take a peek at our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "snp['Close'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Munge the data\n",
    "In the first instance, munging the data is straightforward. The closing prices are of interest, so for convenience extract the closing prices. Because not all of the indices have the same number of values, mainly due to bank holidays, we'll forward-fill the gaps. This means that, if a value isn't available for day N, fill it with the value for another day, such as N-1 or N-2, so that it contains the latest available value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "closing_data = pd.DataFrame()\n",
    "\n",
    "closing_data['snp_close'] = snp['Close']\n",
    "closing_data['nyse_close'] = nyse['Close']\n",
    "closing_data['djia_close'] = djia['Close']\n",
    "closing_data['nikkei_close'] = nikkei['Close']\n",
    "closing_data['hangseng_close'] = hangseng['Close']\n",
    "closing_data['ftse_close'] = ftse['Close']\n",
    "closing_data['dax_close'] = dax['Close']\n",
    "closing_data['aord_close'] = aord['Close']\n",
    "closing_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Pandas includes a very convenient function for filling gaps in the data.\n",
    "closing_data = closing_data.fillna(method='ffill')\n",
    "closing_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore our Data\n",
    "Exploratory Data Analysis (EDA) is foundational to working with machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "_ = pd.concat([closing_data['snp_close'],\n",
    "  closing_data['nyse_close'],\n",
    "  closing_data['djia_close'],\n",
    "  closing_data['nikkei_close'],\n",
    "  closing_data['hangseng_close'],\n",
    "  closing_data['ftse_close'],\n",
    "  closing_data['dax_close'],\n",
    "  closing_data['aord_close']], axis=1).plot(figsize=(20, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Scale the data\n",
    "You can see that the various indices operate on scales differing by orders of magnitude. It's best to scale the data so that, for example, operations involving multiple indices aren't unduly influenced by a single, massive index. Divide each value in an individual index by the maximum value for that index., and then plot. The maximum value of all indices will be 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "closing_data['snp_close_scaled'] = closing_data['snp_close'] / max(closing_data['snp_close'])\n",
    "closing_data['nyse_close_scaled'] = closing_data['nyse_close'] / max(closing_data['nyse_close'])\n",
    "closing_data['djia_close_scaled'] = closing_data['djia_close'] / max(closing_data['djia_close'])\n",
    "closing_data['nikkei_close_scaled'] = closing_data['nikkei_close'] / max(closing_data['nikkei_close'])\n",
    "closing_data['hangseng_close_scaled'] = closing_data['hangseng_close'] / max(closing_data['hangseng_close'])\n",
    "closing_data['ftse_close_scaled'] = closing_data['ftse_close'] / max(closing_data['ftse_close'])\n",
    "closing_data['dax_close_scaled'] = closing_data['dax_close'] / max(closing_data['dax_close'])\n",
    "closing_data['aord_close_scaled'] = closing_data['aord_close'] / max(closing_data['aord_close'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "_ = pd.concat([closing_data['snp_close_scaled'],\n",
    "  closing_data['nyse_close_scaled'],\n",
    "  closing_data['djia_close_scaled'],\n",
    "  closing_data['nikkei_close_scaled'],\n",
    "  closing_data['hangseng_close_scaled'],\n",
    "  closing_data['ftse_close_scaled'],\n",
    "  closing_data['dax_close_scaled'],\n",
    "  closing_data['aord_close_scaled']], axis=1).plot(figsize=(20, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You can see that, over the five-year period, these indices are correlated. Notice that sudden drops from economic events happened globally to all indices, and they otherwise exhibited general rises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Stationary Timeseries with no Trends\n",
    "The actual value of an index is not that useful for modeling. It can be a useful indicator, but to get to the heart of the matter, we need a time series that is stationary in the mean, thus having no trend in the data. A stationary time series is one whose statistical properties such as mean, variance, autocorrelation, etc. are all constant over time. If the series is consistently increasing over time, the sample mean and variance will grow with the size of the sample, and they will always underestimate the mean and variance in future periods. \n",
    "\n",
    "There are various ways of doing that, but they all essentially look at the difference between values, rather than the absolute value. In the case of market data, the usual practice is to work with logged returns, calculated as the natural logarithm of the index today divided by the index yesterday:\n",
    "\n",
    "`ln(Vt/Vt-1)`\n",
    "\n",
    "There are more reasons why the log return is preferable to the percent return (for example the log is normally distributed and additive), but they don't matter much for this work. What matters is to get to a stationary time series.\n",
    "\n",
    "Calculate and plot the log returns in a new DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "log_return_data = pd.DataFrame()\n",
    "\n",
    "log_return_data['snp_log_return'] = np.log(closing_data['snp_close']/closing_data['snp_close'].shift())\n",
    "log_return_data['nyse_log_return'] = np.log(closing_data['nyse_close']/closing_data['nyse_close'].shift())\n",
    "log_return_data['djia_log_return'] = np.log(closing_data['djia_close']/closing_data['djia_close'].shift())\n",
    "log_return_data['nikkei_log_return'] = np.log(closing_data['nikkei_close']/closing_data['nikkei_close'].shift())\n",
    "log_return_data['hangseng_log_return'] = np.log(closing_data['hangseng_close']/closing_data['hangseng_close'].shift())\n",
    "log_return_data['ftse_log_return'] = np.log(closing_data['ftse_close']/closing_data['ftse_close'].shift())\n",
    "log_return_data['dax_log_return'] = np.log(closing_data['dax_close']/closing_data['dax_close'].shift())\n",
    "log_return_data['aord_log_return'] = np.log(closing_data['aord_close']/closing_data['aord_close'].shift())\n",
    "\n",
    "log_return_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "_ = pd.concat([log_return_data['snp_log_return'],\n",
    "  log_return_data['nyse_log_return'],\n",
    "  log_return_data['djia_log_return'],\n",
    "  log_return_data['nikkei_log_return'],\n",
    "  log_return_data['hangseng_log_return'],\n",
    "  log_return_data['ftse_log_return'],\n",
    "  log_return_data['dax_log_return'],\n",
    "  log_return_data['aord_log_return']], axis=1).plot(figsize=(20, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You now have time series for the indices, stationary in the mean, similarly centered and scaled. That's great! Now start to look for signals to try to predict the close of the S&P 500."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Scatter Plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "_ = scatter_matrix(log_return_data, figsize=(20, 20), diagonal='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Summary of our Exploratory Data Analysis\n",
    "\n",
    "At this point, you've done a good enough job of exploratory data analysis. You've visualized our data and come to know it better. You've transformed it into a form that is useful for modelling, log returns, and looked at how indices relate to each other. You've seen that indices from Europe strongly correlate with US indices, and that indices from Asia/Oceania significantly correlate with those same indices for a given day. You've also seen that if you look at historical values, they do not correlate with today's values. Summing up:\n",
    "\n",
    "* European indices from the same day were a strong predictor for the S&P 500 close.\n",
    "* Asian/Oceanian indices from the same day were a significant predictor for the S&P 500 close.\n",
    "* Indices from previous days were not good predictors for the S&P close.\n",
    "\n",
    "What should we think so far?\n",
    "\n",
    "Cloud Datalab is working great. With just a few lines of code, you were able to munge the data, visualize the changes, and make decisions. You could easily analyze and iterate. This is a common feature of iPython, but the advantage here is that Cloud Datalab is a managed service that you can simply click and use, so you can focus on your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Machine Learning - Predict S&P 500 close today will be higher or lower than yesterday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "At this point, we can see a model:\n",
    "\n",
    "* We'll predict whether the S&P 500 close today will be higher or lower than yesterday.\n",
    "* We'll use all our data sources: NYSE, DJIA, Nikkei, Hang Seng, FTSE, DAX, AORD.\n",
    "* We'll use three sets of data points —  T, T-1, and T-2 — where we take the data available on day T or T-n, meaning today's non-US data and yesterday's US data.\n",
    "\n",
    "Predicting whether the log return of the S&P 500 is positive or negative is a classification problem. That is, we want to choose one option from a finite set of options, in this case positive or negative. This is the base case of classification where we have only two values to choose from, known as binary classification, or logistic regression.\n",
    "\n",
    "This uses the findings from of our exploratory data analysis, namely that log returns from other regions on a given day are strongly correlated with the log return of the S&P 500, and there are stronger correlations from those regions that are geographically closer with respect to time zones. However, our models also use data outside of those findings. For example, we use data from the past few days in addition to today.  There are two reasons for using this additional data. First, we're adding additional features to our model for the purpose of this solution to see how things perform. which is not a good reason to add features outside of a tutorial setting. Second, machine learning models are very good at finding weak signals from data.\n",
    "\n",
    "In machine learning, as in most things, there are subtle tradeoffs happening, but in general good data is better than good algorithms, which are better than good frameworks. You need all three pillars but in that order of importance: data, algorithms, frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## TensorFlow\n",
    "[TensorFlow](https://tensorflow.org) is an open source software library, initiated by Google, for numerical computation using data flow graphs. TensorFlow is based on Google's machine learning expertise and is the next generation framework used internally at Google for tasks such as translation and image recognition. It's a wonderful framework for machine learning because it's expressive, efficient, and easy to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Feature engineering for [TensorFlow](https://www.tensorflow.org/)\n",
    "From a training and testing perspective, time series data is easy. Training data should come from events that happened before test data events, and be contiguous in time.  Otherwise,  your model would be trained on events from \"the future\", at least as compared to the test data. It would then likely perform badly in practice, because you can’t really have access to data from the future. That means random sampling or cross validation don't apply to time series data. Decide on a training-versus-testing split, and divide your data into training and test datasets.\n",
    "\n",
    "In this case, you'll create the features together with two additional columns:\n",
    "\n",
    "*  snp_log_return_positive, which is 1 if the log return of the S&P 500 close is positive, and 0 otherwise. \n",
    "* snp_log_return_negative, which is 1 if the log return of the S&P 500 close is negative, and 1 otherwise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Prepare our Input Data to TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "log_return_data['snp_log_return_positive'] = 0\n",
    "log_return_data.ix[log_return_data['snp_log_return'] >= 0, 'snp_log_return_positive'] = 1\n",
    "log_return_data['snp_log_return_negative'] = 0\n",
    "log_return_data.ix[log_return_data['snp_log_return'] < 0, 'snp_log_return_negative'] = 1\n",
    "\n",
    "training_test_data = pd.DataFrame(\n",
    "  columns=[\n",
    "    'snp_log_return_positive', 'snp_log_return_negative',\n",
    "    'snp_log_return_1', 'snp_log_return_2', 'snp_log_return_3',\n",
    "    'nyse_log_return_1', 'nyse_log_return_2', 'nyse_log_return_3',\n",
    "    'djia_log_return_1', 'djia_log_return_2', 'djia_log_return_3',\n",
    "    'nikkei_log_return_0', 'nikkei_log_return_1', 'nikkei_log_return_2',\n",
    "    'hangseng_log_return_0', 'hangseng_log_return_1', 'hangseng_log_return_2',\n",
    "    'ftse_log_return_0', 'ftse_log_return_1', 'ftse_log_return_2',\n",
    "    'dax_log_return_0', 'dax_log_return_1', 'dax_log_return_2',\n",
    "    'aord_log_return_0', 'aord_log_return_1', 'aord_log_return_2'])\n",
    "\n",
    "for i in range(7, len(log_return_data)):\n",
    "  snp_log_return_positive = log_return_data['snp_log_return_positive'].ix[i]\n",
    "  snp_log_return_negative = log_return_data['snp_log_return_negative'].ix[i]\n",
    "  snp_log_return_1 = log_return_data['snp_log_return'].ix[i-1]\n",
    "  snp_log_return_2 = log_return_data['snp_log_return'].ix[i-2]\n",
    "  snp_log_return_3 = log_return_data['snp_log_return'].ix[i-3]\n",
    "  nyse_log_return_1 = log_return_data['nyse_log_return'].ix[i-1]\n",
    "  nyse_log_return_2 = log_return_data['nyse_log_return'].ix[i-2]\n",
    "  nyse_log_return_3 = log_return_data['nyse_log_return'].ix[i-3]\n",
    "  djia_log_return_1 = log_return_data['djia_log_return'].ix[i-1]\n",
    "  djia_log_return_2 = log_return_data['djia_log_return'].ix[i-2]\n",
    "  djia_log_return_3 = log_return_data['djia_log_return'].ix[i-3]\n",
    "  nikkei_log_return_0 = log_return_data['nikkei_log_return'].ix[i]\n",
    "  nikkei_log_return_1 = log_return_data['nikkei_log_return'].ix[i-1]\n",
    "  nikkei_log_return_2 = log_return_data['nikkei_log_return'].ix[i-2]\n",
    "  hangseng_log_return_0 = log_return_data['hangseng_log_return'].ix[i]\n",
    "  hangseng_log_return_1 = log_return_data['hangseng_log_return'].ix[i-1]\n",
    "  hangseng_log_return_2 = log_return_data['hangseng_log_return'].ix[i-2]\n",
    "  ftse_log_return_0 = log_return_data['ftse_log_return'].ix[i]\n",
    "  ftse_log_return_1 = log_return_data['ftse_log_return'].ix[i-1]\n",
    "  ftse_log_return_2 = log_return_data['ftse_log_return'].ix[i-2]\n",
    "  dax_log_return_0 = log_return_data['dax_log_return'].ix[i]\n",
    "  dax_log_return_1 = log_return_data['dax_log_return'].ix[i-1]\n",
    "  dax_log_return_2 = log_return_data['dax_log_return'].ix[i-2]\n",
    "  aord_log_return_0 = log_return_data['aord_log_return'].ix[i]\n",
    "  aord_log_return_1 = log_return_data['aord_log_return'].ix[i-1]\n",
    "  aord_log_return_2 = log_return_data['aord_log_return'].ix[i-2]\n",
    "  training_test_data = training_test_data.append(\n",
    "    {'snp_log_return_positive':snp_log_return_positive,\n",
    "    'snp_log_return_negative':snp_log_return_negative,\n",
    "    'snp_log_return_1':snp_log_return_1,\n",
    "    'snp_log_return_2':snp_log_return_2,\n",
    "    'snp_log_return_3':snp_log_return_3,\n",
    "    'nyse_log_return_1':nyse_log_return_1,\n",
    "    'nyse_log_return_2':nyse_log_return_2,\n",
    "    'nyse_log_return_3':nyse_log_return_3,\n",
    "    'djia_log_return_1':djia_log_return_1,\n",
    "    'djia_log_return_2':djia_log_return_2,\n",
    "    'djia_log_return_3':djia_log_return_3,\n",
    "    'nikkei_log_return_0':nikkei_log_return_0,\n",
    "    'nikkei_log_return_1':nikkei_log_return_1,\n",
    "    'nikkei_log_return_2':nikkei_log_return_2,\n",
    "    'hangseng_log_return_0':hangseng_log_return_0,\n",
    "    'hangseng_log_return_1':hangseng_log_return_1,\n",
    "    'hangseng_log_return_2':hangseng_log_return_2,\n",
    "    'ftse_log_return_0':ftse_log_return_0,\n",
    "    'ftse_log_return_1':ftse_log_return_1,\n",
    "    'ftse_log_return_2':ftse_log_return_2,\n",
    "    'dax_log_return_0':dax_log_return_0,\n",
    "    'dax_log_return_1':dax_log_return_1,\n",
    "    'dax_log_return_2':dax_log_return_2,\n",
    "    'aord_log_return_0':aord_log_return_0,\n",
    "    'aord_log_return_1':aord_log_return_1,\n",
    "    'aord_log_return_2':aord_log_return_2},\n",
    "    ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Split our Data for Training & Testing\n",
    "We'll use 80% of our data for training and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "training_set_size = int(len(training_test_data) * 0.8)\n",
    "test_set_size = len(training_test_data) - training_set_size\n",
    "data_train = training_test_data[:training_set_size]\n",
    "data_test = training_test_data[training_set_size:]\n",
    "print \"Training Data Matrix:\",data_train.shape\n",
    "print \"Testing Data Matrix:\",data_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data Staging Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "GCS_BUCKET = 'gs://demo_fox_06_22' #CHANGE THIS TO YOUR BUCKET\n",
    "PROJECT = 'ml-workshop-198917' #CHANGE THIS TO YOUR PROJECT ID\n",
    "REGION = 'us-central1' #OPTIONALLY CHANGE THIS\n",
    "import os\n",
    "os.environ['GCS_BUCKET'] = GCS_BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_train.to_csv(\"snp_train.csv\", header=False)\n",
    "data_test.to_csv(\"snp_test.csv\", header=False)\n",
    "\n",
    "#TODO: \"header = false\" works only for string headers for pandas\n",
    "!sed -i '1d' snp_train.csv # Dropping the header line of the CSV since header = false is failing\n",
    "!sed -i '1d' snp_test.csv  # Dropping the header line of the CSV \n",
    "\n",
    "!gsutil mb $GCS_BUCKET\n",
    "!gsutil cp snp_train.csv snp_test.csv $GCS_BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Building a Machine Learning Model with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir trainer\n",
    "touch trainer/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%writefile trainer/task.py\n",
    "\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn import learn_runner\n",
    "from tensorflow.contrib.learn.python.learn.utils import saved_model_export_utils\n",
    "from tensorflow.contrib.learn.python.learn.utils import input_fn_utils\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "\n",
    "data_train = pd.read_csv(\n",
    "  filepath_or_buffer= 'https://storage.googleapis.com/stock_indices_snp_prediction/snp_train.csv',\n",
    "  names=['snp_log_return_positive', 'snp_log_return_negative',\n",
    "    'snp_log_return_1', 'snp_log_return_2', 'snp_log_return_3',\n",
    "    'nyse_log_return_1', 'nyse_log_return_2', 'nyse_log_return_3',\n",
    "    'djia_log_return_1', 'djia_log_return_2', 'djia_log_return_3',\n",
    "    'nikkei_log_return_0', 'nikkei_log_return_1', 'nikkei_log_return_2',\n",
    "    'hangseng_log_return_0', 'hangseng_log_return_1', 'hangseng_log_return_2',\n",
    "    'ftse_log_return_0', 'ftse_log_return_1', 'ftse_log_return_2',\n",
    "    'dax_log_return_0', 'dax_log_return_1', 'dax_log_return_2',\n",
    "    'aord_log_return_0', 'aord_log_return_1', 'aord_log_return_2'])\n",
    "\n",
    "data_test = pd.read_csv(\n",
    "  filepath_or_buffer= 'https://storage.googleapis.com/stock_indices_snp_prediction/snp_test.csv',\n",
    "  names=['snp_log_return_positive', 'snp_log_return_negative',\n",
    "    'snp_log_return_1', 'snp_log_return_2', 'snp_log_return_3',\n",
    "    'nyse_log_return_1', 'nyse_log_return_2', 'nyse_log_return_3',\n",
    "    'djia_log_return_1', 'djia_log_return_2', 'djia_log_return_3',\n",
    "    'nikkei_log_return_0', 'nikkei_log_return_1', 'nikkei_log_return_2',\n",
    "    'hangseng_log_return_0', 'hangseng_log_return_1', 'hangseng_log_return_2',\n",
    "    'ftse_log_return_0', 'ftse_log_return_1', 'ftse_log_return_2',\n",
    "    'dax_log_return_0', 'dax_log_return_1', 'dax_log_return_2',\n",
    "    'aord_log_return_0', 'aord_log_return_1', 'aord_log_return_2'])\n",
    "\n",
    "FEATURES = [\n",
    "    'snp_log_return_1', 'snp_log_return_2', 'snp_log_return_3',\n",
    "    'nyse_log_return_1', 'nyse_log_return_2', 'nyse_log_return_3',\n",
    "    'djia_log_return_1', 'djia_log_return_2', 'djia_log_return_3',\n",
    "    'nikkei_log_return_0', 'nikkei_log_return_1', 'nikkei_log_return_2',\n",
    "    'hangseng_log_return_0', 'hangseng_log_return_1', 'hangseng_log_return_2',\n",
    "    'ftse_log_return_0', 'ftse_log_return_1', 'ftse_log_return_2',\n",
    "    'dax_log_return_0', 'dax_log_return_1', 'dax_log_return_2',\n",
    "    'aord_log_return_0', 'aord_log_return_1', 'aord_log_return_2']\n",
    "\n",
    "\n",
    "LABEL = 'snp_log_return_positive'\n",
    "\n",
    "feature_cols = [tf.contrib.layers.real_valued_column(k)\n",
    "                  for k in FEATURES]\n",
    "\n",
    "def generate_estimator(output_dir):\n",
    "  return tf.estimator.DNNClassifier(feature_columns=feature_cols,\n",
    "                                            hidden_units=[50, 25],\n",
    "                                            model_dir=output_dir)\n",
    "\n",
    "def generate_input_fn(data_set):\n",
    "    def input_fn():\n",
    "      features = {k: tf.constant(data_set[k].values) for k in FEATURES}\n",
    "      labels = tf.constant(data_set[LABEL].values)\n",
    "      return features, labels\n",
    "    return input_fn\n",
    "  \n",
    "\n",
    "def serving_input_fn():\n",
    "  #feature_placeholders are what the caller of the predict() method will have to provide\n",
    "  feature_placeholders = {\n",
    "      column.name: tf.placeholder(column.dtype, [None])\n",
    "      for column in feature_cols\n",
    "  }\n",
    "  # DNNCombinedLinearClassifier expects rank 2 Tensors, but inputs should be\n",
    "  # rank 1, so that we can provide scalars to the server\n",
    "  features = {\n",
    "    key: tf.expand_dims(tensor, -1)\n",
    "    for key, tensor in feature_placeholders.items()\n",
    "  }\n",
    "  return tf.estimator.export.ServingInputReceiver(\n",
    "           features, feature_placeholders)\n",
    "\n",
    "train_spec = tf.estimator.TrainSpec(\n",
    "                input_fn=generate_input_fn(data_train),\n",
    "                max_steps=3000)\n",
    "\n",
    "exporter = tf.estimator.LatestExporter('Servo', serving_input_fn)\n",
    "\n",
    "eval_spec=tf.estimator.EvalSpec(\n",
    "            input_fn=generate_input_fn(data_test),\n",
    "            steps=1,\n",
    "            exporters=exporter)\n",
    "\n",
    "######CLOUD ML ENGINE BOILERPLATE CODE BELOW######\n",
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser()\n",
    "  # Input Arguments\n",
    "  parser.add_argument(\n",
    "      '--output_dir',\n",
    "      help='GCS location to write checkpoints and export models',\n",
    "      required=True\n",
    "  )\n",
    "  parser.add_argument(\n",
    "        '--job-dir',\n",
    "        help='this model ignores this field, but it is required by gcloud',\n",
    "        default='junk'\n",
    "    )\n",
    "  args = parser.parse_args()\n",
    "  arguments = args.__dict__\n",
    "  output_dir = arguments.pop('output_dir')\n",
    "  \n",
    "######END OF CLOUD ML ENGINE BOILERPLATE CODE######\n",
    "\n",
    "  tf.estimator.train_and_evaluate(generate_estimator(output_dir), train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train Locally to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud ml-engine local train \\\n",
    "   --module-name=trainer.task \\\n",
    "   --package-path=trainer \\\n",
    "   -- \\\n",
    "   --output_dir='./output_snp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train via  on Cloud ML Engine\n",
    "Below we will launch a remore distributed training job on Cloud Machine Learning Engine (Cloud ML). To learn more on how to run the job please visit this [link](https://cloud.google.com/ml-engine/docs/training-overview) Please check your job status [here](https://console.cloud.google.com/mlengine/jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "JOBNAME=snp_predict_$(date -u +%y%m%d_%H%M%S)\n",
    "\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "   --region=$REGION \\\n",
    "   --module-name=trainer.task \\\n",
    "   --package-path=./trainer \\\n",
    "   --job-dir=$GCS_BUCKET/$JOBNAME \\\n",
    "   --runtime-version 1.4 \\\n",
    "   --scale-tier=BASIC \\\n",
    "   -- \\\n",
    "   --output_dir=$GCS_BUCKET/$JOBNAME/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "JOBNAME=snp_predict_$(date -u +%y%m%d_%H%M%S)\n",
    "\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "   --region=$REGION \\\n",
    "   --module-name=trainer.task \\\n",
    "   --package-path=./trainer \\\n",
    "   --job-dir=$GCS_BUCKET/$JOBNAME \\\n",
    "   --scale-tier=STANDARD_1 \\\n",
    "   -- \\\n",
    "   --output_dir=$GCS_BUCKET/$JOBNAME/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Running on 8 GPUs\n",
    "To train across multiple GPUs you use a [custom scale tier](https://cloud.google.com/ml-engine/docs/concepts/training-overview#job_configuration_parameters). Here I am specifying a master node with machine type complex_model_m_gpu and one worker node of the same type. Each complex_model_m_gpu has 4 GPUs so this job will run on 2x4=8 GPUs total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%writefile config.yaml\n",
    "trainingInput:\n",
    "  scaleTier: CUSTOM\n",
    "  masterType: complex_model_m_gpu\n",
    "  workerType: complex_model_m_gpu\n",
    "  workerCount: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "JOBNAME=housing_$(date -u +%y%m%d_%H%M%S)\n",
    "\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "   --region=$REGION \\\n",
    "   --module-name=trainer.task \\\n",
    "   --package-path=./trainer \\\n",
    "   --job-dir=$GCS_BUCKET/$JOBNAME \\\n",
    "   --config config.yaml \\\n",
    "   -- \\\n",
    "   --output_dir=$GCS_BUCKET/$JOBNAME/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## TensorBoard - View our Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from google.datalab.ml import TensorBoard\n",
    "TensorBoard().start('gs://demo_fox_06_22/snp_predict_180622_214308/output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for pid in TensorBoard.list()['pid']:\n",
    "  TensorBoard().stop(pid)\n",
    "  print 'Stopped TensorBoard with pid {}'.format(pid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Deploying the Model to CloudML Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "MODEL_NAME=\"snp_predict_cat\"\n",
    "MODEL_VERSION=\"v1\"\n",
    "MODEL_LOCATION=\"gs://demo_fox_06_22/snp_predict_180622_214308/output/export/Servo/1529703898/\" #REPLACE this with the location of your model\n",
    "\n",
    "gcloud ml-engine models create ${MODEL_NAME} --regions $REGION\n",
    "gcloud ml-engine versions create ${MODEL_VERSION} --model ${MODEL_NAME} --runtime-version=1.4 --origin ${MODEL_LOCATION} --staging-bucket=$GCS_BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Predicting SnP Index via REST API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%writefile records.json\n",
    "{\"snp_log_return_1\": 0.03444344508, \"snp_log_return_2\": 0.1446582898,\"snp_log_return_3\": 0.3705232341,\"nyse_log_return_1\": 0.02456562007,\"nyse_log_return_2\": 0.1250913529,\"nyse_log_return_3\": 0.2642337233,\"djia_log_return_1\": 0.0182481161,\"djia_log_return_2\": 0.08448574603,\"djia_log_return_3\": 0.3487944987,\"nikkei_log_return_0\": 0.1839209456,\"nikkei_log_return_1\": 0.05921528338,\"nikkei_log_return_2\": 0.07425687362,\"hangseng_log_return_0\": -0.02200372952,\"hangseng_log_return_1\": 0.06268626658,\"hangseng_log_return_2\": 0.02025181673,\"ftse_log_return_0\": -0.01264577313,\"ftse_log_return_1\": -0.008332741532,\"ftse_log_return_2\": 0.02783490249,\"dax_log_return_0\": 0.180807621,\"dax_log_return_1\": -0.03507709645,\"dax_log_return_2\": 0.168394005,\"aord_log_return_0\": 0.03949755919,\"aord_log_return_1\": 0.01430315398,\"aord_log_return_2\": 0.1103549831}\n",
    "{\"snp_log_return_1\": 0.2375707945, \"snp_log_return_2\": 0.04584539109,\"snp_log_return_3\": -0.04549661146,\"nyse_log_return_1\": 0.202237408,\"nyse_log_return_2\": 0.04664838585,\"nyse_log_return_3\": -0.03361422942,\"djia_log_return_1\": 0.1951068622,\"djia_log_return_2\": 0.0323228083,\"djia_log_return_3\": -0.03737675297,\"nikkei_log_return_0\": -0.1848016978,\"nikkei_log_return_1\": 0.5660431288,\"nikkei_log_return_2\": -0.04978258407,\"hangseng_log_return_0\": -0.04915084218,\"hangseng_log_return_1\": 0.05009520323,\"hangseng_log_return_2\": 0.05139190931,\"ftse_log_return_0\": -0.05119295178,\"ftse_log_return_1\": 0.1160193433,\"ftse_log_return_2\": 0.03751350795,\"dax_log_return_0\": -0.2043722151,\"dax_log_return_1\": 0.249019381,\"dax_log_return_2\": 0.1008303821,\"aord_log_return_0\": -0.06859242867,\"aord_log_return_1\": 0.1420852487,\"aord_log_return_2\": 0.04071157064}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!gcloud ml-engine predict --model snp_predict_cat --json-instances records.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "You've covered a lot of ground. You moved from sourcing five years of financial time-series data, to munging that data into a more suitable form. You explored and visualized that data with exploratory data analysis and then decided on a machine learning model and the features for that model. You engineered those features, built a binary classifier in TensorFlow, and analyzed its performance. You built a feed forward neural net with two hidden layers in TensorFlow and analyzed its performance.\n",
    "\n",
    "How did the technology fare? It should take most people 1.5 to 3 hours to extract the juice from this solution, and none of that time is spent waiting for infrastructure or software; it's spent reading and thinking. In many organizations, it can take anywhere from days to months to do this sort of data analysis, depending on whether you need to procure any hardware. And you didn't need to do anything with infrastructure or additional software. Rather, you used a web-based console to direct GCP to set up systems on your behalf, which it did—fully managed, maintained, and supported—freeing you up to spend your time analyzing. \n",
    "\n",
    "It was also cost effective. If you took your time with this solution and spent three hours to go through it, the cost would be a few pennies. \n",
    "\n",
    "Cloud Datalab worked admirably, too. iPython/Jupyter has always been a great platform for interactive, iterative work and a fully-managed version of that platform on GCP, with connectors to other GCP technologies such as BigQuery and Google Cloud Storage, is a force multiplier for your analysis needs.  If you haven't used iPython before, this solution might have been eye opening, for you. If you're already familiar with iPython, then you'll love the connectors to other GCP technologies.\n",
    "\n",
    "Of course, R and Matlab are popular tools in machine learning, and we've made no mention either in this solution. Neither R nor Matlab are available as managed services on GCP. Both can be hosted in GCP and accessed through a cloud-friendly, web frontend.\n",
    "\n",
    "TensorFlow is a special piece of technology. It is expressive, performs well, and comes with the weight of Google's machine learning history and expertise to back it up and support it. We've only scratched the surface, but you can already see that within a handful of lines of code we've been able to write two models. Neither of them is cutting edge, by design, but neither of them is trivial either. With some additional tuning they would suit a whole spectrum of machine learning tasks. \n",
    "\n",
    "Finally, how did we do with the data analysis? We did well: over 70% accuracy in predicting the close of the S&P 500 is the highest we've seen achieved on this dataset, so with few steps and a few lines of code we've produced a full-on machine learning model. The reason for the relatively modest accuracy achieved is the dataset itself; there isn't enough signal there to do significantly better. But 7 times out of 10, we were able to correctly determine if the S&P 500 index would close up or down on the day, and that's objectively good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Bonus - Hyperparamter Tuning\n",
    "Hyperparameter tuning in [Cloud Machine Learning Engine using Bayesian Optimization](https://cloud.google.com/blog/big-data/2017/08/hyperparameter-tuning-in-cloud-machine-learning-engine-using-bayesian-optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.yaml\n",
    "trainingInput:\n",
    "  hyperparameters:\n",
    "    goal: MINIMIZE\n",
    "    hyperparameterMetricTag: accuracy\n",
    "    maxTrials: 2\n",
    "    maxParallelTrials: 1\n",
    "    params:\n",
    "     - parameterName: learning_rate\n",
    "       type: INTEGER\n",
    "       minValue: 20\n",
    "       maxValue: 70\n",
    "       scaleType: UNIT_LINEAR_SCALE\n",
    "     - parameterName: num_layers\n",
    "       type: INTEGER\n",
    "       minValue: 2\n",
    "       maxValue: 4\n",
    "       scaleType: UNIT_LINEAR_SCALE\n",
    "     - parameterName: scale_factor\n",
    "       type: DOUBLE\n",
    "       minValue: 0.1\n",
    "       maxValue: 0.8\n",
    "       scaleType: UNIT_REVERSE_LOG_SCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%writefile trainer/task.py\n",
    "\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn import learn_runner\n",
    "from tensorflow.contrib.learn.python.learn.utils import saved_model_export_utils\n",
    "from tensorflow.contrib.learn.python.learn.utils import input_fn_utils\n",
    "from tensorflow.contrib.training.python.training import hparam\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "\n",
    "data_train = pd.read_csv(\n",
    "  filepath_or_buffer= 'https://storage.googleapis.com/stock_indices_snp_prediction/snp_train.csv',\n",
    "  names=['snp_log_return_positive', 'snp_log_return_negative',\n",
    "    'snp_log_return_1', 'snp_log_return_2', 'snp_log_return_3',\n",
    "    'nyse_log_return_1', 'nyse_log_return_2', 'nyse_log_return_3',\n",
    "    'djia_log_return_1', 'djia_log_return_2', 'djia_log_return_3',\n",
    "    'nikkei_log_return_0', 'nikkei_log_return_1', 'nikkei_log_return_2',\n",
    "    'hangseng_log_return_0', 'hangseng_log_return_1', 'hangseng_log_return_2',\n",
    "    'ftse_log_return_0', 'ftse_log_return_1', 'ftse_log_return_2',\n",
    "    'dax_log_return_0', 'dax_log_return_1', 'dax_log_return_2',\n",
    "    'aord_log_return_0', 'aord_log_return_1', 'aord_log_return_2'])\n",
    "\n",
    "data_test = pd.read_csv(\n",
    "  filepath_or_buffer= 'https://storage.googleapis.com/stock_indices_snp_prediction/snp_test.csv',\n",
    "  names=['snp_log_return_positive', 'snp_log_return_negative',\n",
    "    'snp_log_return_1', 'snp_log_return_2', 'snp_log_return_3',\n",
    "    'nyse_log_return_1', 'nyse_log_return_2', 'nyse_log_return_3',\n",
    "    'djia_log_return_1', 'djia_log_return_2', 'djia_log_return_3',\n",
    "    'nikkei_log_return_0', 'nikkei_log_return_1', 'nikkei_log_return_2',\n",
    "    'hangseng_log_return_0', 'hangseng_log_return_1', 'hangseng_log_return_2',\n",
    "    'ftse_log_return_0', 'ftse_log_return_1', 'ftse_log_return_2',\n",
    "    'dax_log_return_0', 'dax_log_return_1', 'dax_log_return_2',\n",
    "    'aord_log_return_0', 'aord_log_return_1', 'aord_log_return_2'])\n",
    "\n",
    "FEATURES = [\n",
    "    'snp_log_return_1', 'snp_log_return_2', 'snp_log_return_3',\n",
    "    'nyse_log_return_1', 'nyse_log_return_2', 'nyse_log_return_3',\n",
    "    'djia_log_return_1', 'djia_log_return_2', 'djia_log_return_3',\n",
    "    'nikkei_log_return_0', 'nikkei_log_return_1', 'nikkei_log_return_2',\n",
    "    'hangseng_log_return_0', 'hangseng_log_return_1', 'hangseng_log_return_2',\n",
    "    'ftse_log_return_0', 'ftse_log_return_1', 'ftse_log_return_2',\n",
    "    'dax_log_return_0', 'dax_log_return_1', 'dax_log_return_2',\n",
    "    'aord_log_return_0', 'aord_log_return_1', 'aord_log_return_2']\n",
    "\n",
    "\n",
    "LABEL = 'snp_log_return_positive'\n",
    "\n",
    "feature_cols = [tf.contrib.layers.real_valued_column(k)\n",
    "                  for k in FEATURES]\n",
    "\n",
    "def generate_estimator(output_dir):\n",
    "  return tf.estimator.DNNClassifier(feature_columns=feature_cols,\n",
    "                                    #hidden_units=[args.hidden_units_1, args.hidden_units_2], #NEW (use command line parameters for hidden units)\n",
    "                                    hidden_units=[max(2, int(args.hidden_units_1 * args.scale_factor**i)) \n",
    "                                                      for i in range(args.num_layers)],\n",
    "                                            model_dir=output_dir)\n",
    "\n",
    "def generate_input_fn(data_set):\n",
    "    def input_fn():\n",
    "      features = {k: tf.constant(data_set[k].values) for k in FEATURES}\n",
    "      labels = tf.constant(data_set[LABEL].values)\n",
    "      return features, labels\n",
    "    return input_fn\n",
    "  \n",
    "\n",
    "def serving_input_fn():\n",
    "  #feature_placeholders are what the caller of the predict() method will have to provide\n",
    "  feature_placeholders = {\n",
    "      column.name: tf.placeholder(column.dtype, [None])\n",
    "      for column in feature_cols\n",
    "  }\n",
    "  # DNNCombinedLinearClassifier expects rank 2 Tensors, but inputs should be\n",
    "  # rank 1, so that we can provide scalars to the server\n",
    "  features = {\n",
    "    key: tf.expand_dims(tensor, -1)\n",
    "    for key, tensor in feature_placeholders.items()\n",
    "  }\n",
    "  return tf.estimator.export.ServingInputReceiver(\n",
    "           features, feature_placeholders)\n",
    "\n",
    "train_spec = tf.estimator.TrainSpec(\n",
    "                input_fn=generate_input_fn(data_train),\n",
    "                max_steps=3000)\n",
    "\n",
    "exporter = tf.estimator.LatestExporter('Servo', serving_input_fn)\n",
    "\n",
    "eval_spec=tf.estimator.EvalSpec(\n",
    "            input_fn=generate_input_fn(data_test),\n",
    "            steps=1,\n",
    "            exporters=exporter)\n",
    "\n",
    "######CLOUD ML ENGINE BOILERPLATE CODE BELOW######\n",
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser()\n",
    "\n",
    "  parser.add_argument(\n",
    "      '--output_dir',\n",
    "      help='GCS location to write checkpoints and export models',\n",
    "      required=True\n",
    "  )\n",
    "  parser.add_argument(\n",
    "        '--job-dir',\n",
    "        help='this model ignores this field, but it is required by gcloud',\n",
    "        default='junk'\n",
    "    )\n",
    "  parser.add_argument(\n",
    "        '--hidden_units_1', #NEW (expose hyperparameter to command line)\n",
    "        help='Number of nodes in the hidden layer of the DNN',\n",
    "        default=40,\n",
    "        type=int\n",
    "    )\n",
    "  parser.add_argument(\n",
    "      '--num_layers',\n",
    "      help='Number of layers in the DNN',\n",
    "      default=2,\n",
    "      type=int\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--scale_factor',\n",
    "      help='How quickly should the size of the layers in the DNN decay',\n",
    "      default=0.7,\n",
    "      type=float\n",
    "  )\n",
    "  args = parser.parse_args()\n",
    "  arguments = args.__dict__\n",
    "  output_dir = arguments.pop('output_dir')\n",
    "\n",
    "  #initiate training job\n",
    "  tf.estimator.train_and_evaluate(generate_estimator(output_dir), train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "JOBNAME=snp_predict_with_hp$(date -u +%y%m%d_%H%M%S)\n",
    "\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "   --region=$REGION \\\n",
    "   --module-name=trainer.task \\\n",
    "   --package-path=./trainer \\\n",
    "   --job-dir=$GCS_BUCKET/$JOBNAME \\\n",
    "   --config config.yaml \\\n",
    "   --scale-tier=STANDARD_1 \\\n",
    "   --runtime-version 1.4 \\\n",
    "   -- \\\n",
    "   --output_dir=$GCS_BUCKET/$JOBNAME/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud ml-engine jobs describe  snp_predict_with_hp180623_001014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Cleanup\n",
    "**When you're finished, shut down the managed VM you used for Cloud Datalab to avoid incurring costs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print('''\n",
    "Copyright 2016, Google, Inc.\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
